"""
train.py - Huáº¥n luyá»‡n mÃ´ hÃ¬nh CNN phÃ¢n loáº¡i cÃ  chua (Xanh/ChÃ­n)
TÃ¡c giáº£: Tháº¿ Anh
"""

import os
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau

# ========== Cáº¤U HÃŒNH ==========
IMG_SIZE = (128, 128)  # KÃ­ch thÆ°á»›c áº£nh Ä‘áº§u vÃ o
BATCH_SIZE = 32
EPOCHS = 50
LEARNING_RATE = 0.001

# ÄÆ°á»ng dáº«n dá»¯ liá»‡u
TRAIN_DIR = 'data/train'
VAL_DIR = 'data/val'
MODEL_SAVE_PATH = 'models/best_model.h5'

# Táº¡o thÆ° má»¥c models náº¿u chÆ°a cÃ³
os.makedirs('models', exist_ok=True)

print("="*60)
print("ğŸ… Báº®T Äáº¦U HUáº¤N LUYá»†N MÃ” HÃŒNH CNN PHÃ‚N LOáº I CÃ€ CHUA")
print("="*60)
print(f"ğŸ“… Thá»i gian: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ğŸ–¼ï¸  KÃ­ch thÆ°á»›c áº£nh: {IMG_SIZE}")
print(f"ğŸ“¦ Batch size: {BATCH_SIZE}")
print(f"ğŸ”„ Sá»‘ epochs: {EPOCHS}")
print(f"ğŸ“ˆ Learning rate: {LEARNING_RATE}")
print("="*60)


# ========== CHUáº¨N Bá»Š Dá»® LIá»†U ==========
print("\nğŸ“‚ Äang táº£i dá»¯ liá»‡u...")

# Data Augmentation cho táº­p train (tÄƒng cÆ°á»ng dá»¯ liá»‡u)
train_datagen = ImageDataGenerator(
    rescale=1./255,              # Chuáº©n hÃ³a pixel vá» [0,1]
    rotation_range=20,           # Xoay ngáº«u nhiÃªn Â±20Â°
    width_shift_range=0.2,       # Dá»‹ch ngang 20%
    height_shift_range=0.2,      # Dá»‹ch dá»c 20%
    shear_range=0.2,             # Biáº¿n dáº¡ng nghiÃªng
    zoom_range=0.2,              # Zoom in/out
    horizontal_flip=True,        # Láº­t ngang
    fill_mode='nearest'          # Äiá»n pixel khi biáº¿n Ä‘á»•i
)

# Chá»‰ chuáº©n hÃ³a cho táº­p validation (khÃ´ng augment)
val_datagen = ImageDataGenerator(rescale=1./255)

# Load dá»¯ liá»‡u tá»« thÆ° má»¥c
train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',    # One-hot encoding
    shuffle=True
)

val_generator = val_datagen.flow_from_directory(
    VAL_DIR,
    target_size=IMG_SIZE,
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

# Sá»‘ lÆ°á»£ng class
num_classes = len(train_generator.class_indices)
class_names = list(train_generator.class_indices.keys())

print(f"âœ… Táº£i dá»¯ liá»‡u thÃ nh cÃ´ng!")
print(f"   ğŸŸ¢ Sá»‘ lÆ°á»£ng áº£nh train: {train_generator.samples}")
print(f"   ğŸ”µ Sá»‘ lÆ°á»£ng áº£nh validation: {val_generator.samples}")
print(f"   ğŸ·ï¸  CÃ¡c lá»›p: {class_names}")
print(f"   ğŸ“Š Sá»‘ lá»›p: {num_classes}")


# ========== XÃ‚Y Dá»°NG MÃ” HÃŒNH CNN ==========
print("\nğŸ§  Äang xÃ¢y dá»±ng mÃ´ hÃ¬nh CNN...")

model = models.Sequential([
    # Block 1
    layers.Conv2D(32, (3, 3), activation='relu', padding='same', 
                  input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Block 2
    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Block 3
    layers.Conv2D(128, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    
    # Block 4
    layers.Conv2D(256, (3, 3), activation='relu', padding='same'),
    layers.BatchNormalization(),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.4),
    
    # Fully Connected Layers
    layers.Flatten(),
    layers.Dense(512, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    
    layers.Dense(128, activation='relu'),
    layers.BatchNormalization(),
    layers.Dropout(0.5),
    
    # Output Layer
    layers.Dense(num_classes, activation='softmax')
])

print("âœ… MÃ´ hÃ¬nh CNN Ä‘Ã£ Ä‘Æ°á»£c xÃ¢y dá»±ng!")
model.summary()


# ========== COMPILE MÃ” HÃŒNH ==========
print("\nâš™ï¸  Äang compile mÃ´ hÃ¬nh...")

optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)

model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',  # HÃ m loss cho multi-class
    metrics=['accuracy']
)

print("âœ… Compile hoÃ n táº¥t!")


# ========== CALLBACKS ==========
print("\nğŸ”§ Thiáº¿t láº­p callbacks...")

# 1. Early Stopping - Dá»«ng sá»›m náº¿u khÃ´ng cáº£i thiá»‡n
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

# 2. Model Checkpoint - LÆ°u mÃ´ hÃ¬nh tá»‘t nháº¥t
checkpoint = ModelCheckpoint(
    MODEL_SAVE_PATH,
    monitor='val_accuracy',
    save_best_only=True,
    mode='max',
    verbose=1
)

# 3. Reduce Learning Rate - Giáº£m LR khi plateau
reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)

callbacks = [early_stop, checkpoint, reduce_lr]

print("âœ… Callbacks Ä‘Ã£ sáºµn sÃ ng!")


# ========== HUáº¤N LUYá»†N MÃ” HÃŒNH ==========
print("\n" + "="*60)
print("ğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N MÃ” HÃŒNH")
print("="*60 + "\n")

history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=val_generator,
    callbacks=callbacks,
    verbose=1
)

print("\n" + "="*60)
print("âœ… HUáº¤N LUYá»†N HOÃ€N Táº¤T!")
print("="*60)


# ========== LÆ¯U Káº¾T QUáº¢ ==========
print("\nğŸ’¾ Äang lÆ°u káº¿t quáº£...")

# LÆ°u lá»‹ch sá»­ huáº¥n luyá»‡n
np.save('models/training_history.npy', history.history)

# ÄÃ¡nh giÃ¡ trÃªn táº­p validation
val_loss, val_acc = model.evaluate(val_generator, verbose=0)
print(f"\nğŸ“Š Káº¾T QUáº¢ CUá»I CÃ™NG:")
print(f"   ğŸ”´ Validation Loss: {val_loss:.4f}")
print(f"   ğŸŸ¢ Validation Accuracy: {val_acc*100:.2f}%")


# ========== Váº¼ Äá»’ THá»Š ==========
print("\nğŸ“ˆ Äang táº¡o Ä‘á»“ thá»‹...")

fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Äá»“ thá»‹ Accuracy
axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)
axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Epoch', fontsize=12)
axes[0].set_ylabel('Accuracy', fontsize=12)
axes[0].legend(fontsize=11)
axes[0].grid(True, alpha=0.3)

# Äá»“ thá»‹ Loss
axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)
axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Epoch', fontsize=12)
axes[1].set_ylabel('Loss', fontsize=12)
axes[1].legend(fontsize=11)
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('models/training_history.png', dpi=300, bbox_inches='tight')
print("âœ… Äá»“ thá»‹ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u táº¡i: models/training_history.png")

plt.show()

# ========== Tá»”NG Káº¾T ==========
print("\n" + "="*60)
print("ğŸ‰ HOÃ€N THÃ€NH Táº¤T Cáº¢!")
print("="*60)
print(f"ğŸ“ MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u táº¡i: {MODEL_SAVE_PATH}")
print(f"ğŸ“Š Äá»“ thá»‹ Ä‘Ã£ lÆ°u táº¡i: models/training_history.png")
print(f"ğŸ’¾ Lá»‹ch sá»­ huáº¥n luyá»‡n: models/training_history.npy")
print("="*60)